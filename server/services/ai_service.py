import os
import requests

from langchain_community.llms import Ollama

try:
    from huggingface_hub import InferenceClient
except ImportError:  # huggingface-hub ××•×¤×¦×™×•× ×œ×™
    InferenceClient = None

class AIService:
    def __init__(self):
        self.llm = None
        self.is_active = False
        self.hf_client = None
        self.hf_active = False
        # ×˜×•×§×Ÿ Hugging Face â€“ ×ª×•××š ×’× HF_TOKEN ×•×’× HUGGINGFACEHUB_API_TOKEN
        self.hf_token = os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACEHUB_API_TOKEN")
        # ××•×“×œ ×ª×¨×’×•× ×‘×¨×™×¨×ª ××—×“×œ â€“ ×™×¦×™×‘ ×•× ×ª××š ×‘-HF Inference API
        self.hf_translation_model = os.getenv(
            "HF_TRANSLATION_MODEL", "Helsinki-NLP/opus-mt-en-he"
        )
        
        # × ×™×¡×™×•×Ÿ ×˜×¢×™× ×” "×‘×˜×•×—" - ×œ× ×™×¤×™×œ ××ª ×”×©×¨×ª ×× ×”×¡×¤×¨×™×™×” ×œ× ××•×ª×§× ×ª
        try:
            # ×× ×¡×™× ×œ×™×™×‘× ×¨×§ ×× ×§×™×™×
            from langchain_ollama import OllamaLLM
            self.llm = OllamaLLM(model="llama3")
            self.is_active = True
            print("âœ… AI Service initialized (Ready to connect to Docker/Local Ollama)")
            
        except ImportError:
            print("âš ï¸ langchain-ollama not installed. AI Service running in MOCK mode.")
        except Exception as e:
            print(f"âš ï¸ AI Service warning: {e}. Running in MOCK mode.")

        # --- Hugging Face Inference API ×œ×“×™×¨×•×’ ×—×“×©×•×ª ---
        try:
            if InferenceClient is not None:
                # ××—×–×™×§×™× client (×’× ×œ×“×¨×•×’, ×’× ×œ×ª×¨×’×•× ×× × ×¨×¦×”), ××‘×œ ×œ×“×™×¨×•×’ × ×©×ª××© ×‘-HTTP ×™×©×™×¨
                self.hf_client = InferenceClient(
                    provider="hf-inference",
                    api_key=self.hf_token,
                ) if self.hf_token else None
                self.hf_active = bool(self.hf_token)
                if self.hf_active:
                    print("âœ… HF Client initialized (token detected) for news AI features")
                else:
                    print("âš ï¸ HF token not set â€“ news AI in MOCK mode")
            else:
                print("âš ï¸ huggingface-hub not installed. News ranking in MOCK mode.")
        except Exception as e:
            print(f"âš ï¸ HF Client init failed: {e}. News ranking in MOCK mode.")

    # --- ×¢×–×¨: × ×™×§×•×™ ×˜×§×¡×˜ ××ª×•×¨×’× ××”×§×“××•×ª ××™×•×ª×¨×•×ª ---

    def _clean_translation_output(self, text: str) -> str:
        """×× ×§×” ××”×ª×•×¦××” ×”××ª×•×¨×’××ª ×”×§×“××•×ª ×›××•
        "translate English to Hebrew:" ××• "×ª×¨×’×•× ××× ×’×œ×™×ª ×œ×¢×‘×¨×™×ª:" ×•×›×“'.

        ××—×–×™×¨ ×˜×§×¡×˜ × ×§×™ ×œ×”×¦×’×” ×œ××©×ª××©.
        """
        if not text:
            return text

        t = str(text).strip()
        # ×”×¡×¨×•×ª ×‘×× ×’×œ×™×ª (×× ×”××•×“×œ ×”×“×¤×™×¡ ××ª ×”×¤×¨×•××¤×˜ ×¢×¦××•)
        lower = t.lower()
        english_prefixes = [
            "translate english to hebrew:",
            "translation english to hebrew:",
            "english to hebrew:",
        ]
        for pref in english_prefixes:
            if lower.startswith(pref):
                t = t[len(pref) :].lstrip()
                lower = t.lower()

        # ×”×¡×¨×•×ª ×‘×¢×‘×¨×™×ª â€“ ×•×¨×™××¦×™×•×ª × ×¤×•×¦×•×ª
        hebrew_prefixes = [
            "×ª×¨×’×•× ××× ×’×œ×™×ª ×œ×¢×‘×¨×™×ª",
            "×ª×¨×’× ××× ×’×œ×™×ª ×œ×¢×‘×¨×™×ª",
        ]
        for pref in hebrew_prefixes:
            if t.startswith(pref):
                rest = t[len(pref) :].lstrip()
                if rest.startswith(":"):
                    rest = rest[1:].lstrip()
                t = rest
                break

        return t.strip()

    def analyze_stock(self, symbol: str, price: float) -> str:
        # ×× ××™×Ÿ ×œ× ×• ××•×“×œ ×××™×ª×™, × ×—×–×™×¨ ×ª×©×•×‘×ª ×“××”
        if not self.is_active or not self.llm:
            return (f"[MOCK Analysis] The AI service is currently optimized via Docker. "
                    f"Mock insight: {symbol} at ${price} shows stable trends. "
                    f"Consider market volatility before investing.")

        # × ×™×¡×™×•×Ÿ ×¤× ×™×™×” ×œ××•×“×œ ×”×××™×ª×™ (×× ×”×•× ××•×ª×§×Ÿ)
        try:
            prompt = f"Analyze the stock {symbol} at {price} USD. Is it risky? Answer in 2 short sentences."
            return self.llm.invoke(prompt)
        except Exception as e:
            print(f"âŒ Connection to Ollama failed: {e}")
            return "AI Service is offline. Please check your Docker container."
    
    def generate_investment_plan(self, prompt: str) -> str:
        """
        ×™×¦×™×¨×ª ×ª×›× ×™×ª ×”×©×§×¢×” ××•×ª×××ª ××™×©×™×ª ×‘×××¦×¢×•×ª AI
        """
        # ×× ××™×Ÿ ××•×“×œ ×××™×ª×™, × ×—×–×™×¨ ×ª×•×›× ×™×ª ×“××”
        if not self.is_active or not self.llm:
            return """
ğŸ“Š INVESTMENT PLAN - Based on Your Profile

ğŸ¯ Top 5 Stock Recommendations:
1. NVDA (40%) - Nvidia - Strong tech growth, high dividend potential
2. AAPL (25%) - Apple - Stable blue-chip, defensive position
3. JNJ (20%) - Johnson & Johnson - Dividend stock, low risk
4. MSFT (10%) - Microsoft - Cloud growth exposure
5. Cash Reserve (5%) - Liquidity buffer for short-term opportunities

ğŸ“ˆ Expected Performance:
- Annual Return Projection: 6-8% (Conservative approach)
- Risk Level: Low-Medium
- Volatility Index: Moderate

â° Implementation Timeline:
Week 1: Allocate 50% of capital
Week 2-3: Allocate remaining 50%
Monitor quarterly, rebalance annually

âœ… Diversification Score: 85% - Well-balanced across sectors
ğŸ›¡ï¸ Risk Assessment: CONSERVATIVE - Suitable for your profile
            """
        
        # × ×™×¡×™×•×Ÿ ×¤× ×™×™×” ×œ××•×“×œ ×”×××™×ª×™ (×× Ollama ×¨×¥)
        try:
            print(f"ğŸ¤– AI Service: Generating investment plan with Llama3...")
            response = self.llm.invoke(prompt)
            return str(response).strip()
        except Exception as e:
            print(f"âŒ AI Error: {e}")
            print(f"âš ï¸ Falling back to mock mode...")
            return """
ğŸ“Š INVESTMENT PLAN - MOCK MODE (AI Unavailable)

ğŸ¯ Top 5 Stock Recommendations:
1. NVDA (40%) - Nvidia - Strong tech growth
2. AAPL (25%) - Apple - Stable blue-chip
3. JNJ (20%) - Johnson & Johnson - Dividend stock
4. MSFT (10%) - Microsoft - Cloud growth
5. Cash Reserve (5%) - Liquidity buffer

ğŸ“ˆ Expected Performance:
- Annual Return: 6-8%
- Risk Level: Low-Medium
- Diversification: 85%

âš ï¸ Note: This is a mock analysis. Ensure Ollama is running for real AI recommendations.
            """

    def rank_news_for_stock(self, symbol: str, news_items: list[dict]) -> list[dict]:
        """×“×™×¨×•×’ ×—×“×©×•×ª ×œ×¤×™ ×—×©×™×‘×•×ª ×¢×‘×•×¨ ×× ×™×” ××¡×•×™××ª.

        news_items: ×¨×©×™××” ×©×œ ××•×‘×™×™×§×˜×™× ×¢× ×”××¤×ª×—×•×ª: title, summary, url, published_at
        ××—×–×™×¨: ××•×ª×” ×¨×©×™××”, ×¢× ×©×“×” importance_score ×•×××•×™×™× ×ª ××”×›×™ ×—×©×•×‘ ×œ×¤×—×•×ª ×—×©×•×‘.
        """

        # ××¦×‘ MOCK â€“ ×× HF ×œ× ×–××™×Ÿ/××™×Ÿ ×˜×•×§×Ÿ, × ×‘×¦×¢ ×“×™×¨×•×’ ×¤×©×•×˜ ×œ×¤×™ ××•×¨×š ×”×˜×§×¡×˜
        if not self.hf_active or not getattr(self, "self.hf_token", None):
            ranked = []
            for item in news_items:
                text = f"{item.get('title', '')}. {item.get('summary', '')}"
                score = min(len(text) / 400.0, 1.0)  # ×”×¢×¨×›×” ×’×¡×” ×‘×™×Ÿ 0 ×œ-1
                enriched = dict(item)
                enriched["importance_score"] = round(float(score), 3)
                ranked.append(enriched)

            ranked.sort(key=lambda x: x["importance_score"], reverse=True)
            return ranked

        ranked_items: list[dict] = []

        api_url = "https://api-inference.huggingface.co/models/facebook/bart-large-mnli"
        headers = {"Authorization": f"Bearer {self.hf_token}"}
        labels = ["very important", "somewhat important", "not important"]

        for item in news_items:
            text = f"[Ticker: {symbol.upper()}] {item.get('title', '')}. {item.get('summary', '')}".strip()
            if not text:
                continue

            try:
                resp = requests.post(
                    api_url,
                    headers=headers,
                    json={
                        "inputs": text,
                        "parameters": {
                            "candidate_labels": labels,
                            "multi_label": False,
                        },
                    },
                    timeout=25,
                )
                if resp.status_code != 200:
                    raise ValueError(f"status={resp.status_code}, body={resp.text[:100]}")

                data = resp.json()
                label_scores: dict[str, float] = {}
                if isinstance(data, dict) and "labels" in data and "scores" in data:
                    for lbl, sc in zip(data["labels"], data["scores"]):
                        label_scores[lbl] = float(sc)
                else:
                    raise ValueError("Unexpected HF zero-shot result format")

                score = (
                    1.0 * label_scores.get("very important", 0.0)
                    + 0.6 * label_scores.get("somewhat important", 0.0)
                    + 0.1 * label_scores.get("not important", 0.0)
                )

            except Exception as e:
                print(f"âš ï¸ HF ranking failed for news item: {e}")
                # fallback ×‘××§×¨×” ×©×œ ×©×’×™××”: ×“×™×¨×•×’ ×¤×©×•×˜ ×œ×¤×™ ××•×¨×š ×”×˜×§×¡×˜
                score = min(len(text) / 400.0, 1.0)

            enriched = dict(item)
            enriched["importance_score"] = round(float(score), 3)
            ranked_items.append(enriched)

        ranked_items.sort(key=lambda x: x["importance_score"], reverse=True)
        return ranked_items

    # --- ×ª×¨×’×•× ×œ×¢×‘×¨×™×ª ×‘×××¦×¢×•×ª Hugging Face ---

    def _translate_text_to_hebrew(self, text: str) -> str:
        """×ª×¨×’×•× ×˜×§×¡×˜ ××× ×’×œ×™×ª ×œ×¢×‘×¨×™×ª ×“×¨×š Hugging Face Inference API.

        ×‘××§×¨×” ×©×œ ×›×©×œ ××• ×—×•×¡×¨ ×˜×•×§×Ÿ â€“ ××—×–×™×¨ ××ª ×”×˜×§×¡×˜ ×”××§×•×¨×™.
        """
        text = (text or "").strip()
        if not text:
            return text

        token = os.getenv("HUGGINGFACEHUB_API_TOKEN")
        if not token:
            return text

        url = f"https://api-inference.huggingface.co/models/{self.hf_translation_model}"
        headers = {"Authorization": f"Bearer {token}"}

        try:
            resp = requests.post(url, headers=headers, json={"inputs": text}, timeout=20)
            if resp.status_code != 200:
                print(
                    f"âš ï¸ HF translation error: status={resp.status_code}, body={resp.text[:120]}"
                )
                return text

            data = resp.json()
            # ×¤×•×¨××˜ ×˜×™×¤×•×¡×™: [{"translation_text": "..."}]
            if isinstance(data, list) and data and "translation_text" in data[0]:
                return self._clean_translation_output(data[0]["translation_text"])
            # fallback ×œ×¤×•×¨××˜×™× ××—×¨×™×
            if isinstance(data, dict) and "generated_text" in data:
                return self._clean_translation_output(data["generated_text"])
        except Exception as e:
            print(f"âš ï¸ HF translation exception: {e}")

        return text

    def translate_news_items_to_hebrew(self, news_items: list[dict]) -> list[dict]:
        """×”×•×¡×¤×ª ×©×“×•×ª title_he ×•-summary_he ×œ×›×œ ××™×™×˜× ×—×“×©×•×ª.

        ×××•××© ×‘×‘××¥' ××—×“ ×œ×˜×™×™×˜×œ×™× ×•×‘××¥' ××—×“ ×œ×¡×™×›×•××™× ×›×“×™ ×œ×× ×•×¢
        ×¢×©×¨×•×ª ×§×¨×™××•×ª HTTP × ×¤×¨×“×•×ª (×©×¢×œ×•×œ×•×ª "×œ×ª×§×•×¢" ××ª ×”××¤×œ×™×§×¦×™×”).
        """
        if not news_items:
            return []

        # ××ª×¨×’××™× ×¨×§ ××ª ××” ×©×‘×××ª ××¦×™×’×™× (×¢×“ 10 ××™×™×˜××™×)
        limited_items = news_items[:10]

        titles = [(item.get("title") or "").strip() for item in limited_items]
        summaries = [(item.get("summary") or "").strip() for item in limited_items]

        titles_he = self._translate_batch_to_hebrew(titles)
        summaries_he = self._translate_batch_to_hebrew(summaries)

        translated: list[dict] = []
        for idx, item in enumerate(limited_items):
            copy = dict(item)
            copy["title_he"] = titles_he[idx] if idx < len(titles_he) else titles[idx]
            copy["summary_he"] = (
                summaries_he[idx] if idx < len(summaries_he) else summaries[idx]
            )
            translated.append(copy)

        # ×× ×”×™×• ×™×•×ª×¨ ×-10 ××™×™×˜××™×, × ×•×¡×™×£ ××•×ª× ×œ×œ× ×ª×¨×’×•× (×œ× ××•×¦×’×™× ×‘×¤×•×¢×œ ×‘-UI)
        if len(news_items) > len(limited_items):
            for extra in news_items[len(limited_items) :]:
                translated.append(dict(extra))

        return translated

    def _translate_batch_to_hebrew(self, texts: list[str]) -> list[str]:
        """×ª×¨×’×•× ×¨×©×™××ª ×˜×§×¡×˜×™× ×œ×¢×‘×¨×™×ª ×‘×××¦×¢×•×ª InferenceClient.translation.

        ××©×ª××© ×‘××•×“×œ ×˜×§×¡×˜-×œ×˜×§×¡×˜ (×œ××©×œ google-t5/t5-base) ×¢× ×¤×¨×•××¤×˜
        "translate English to Hebrew: ...". ×‘××§×¨×” ×©×œ ×›×©×œ â€“ × ×—×–×™×¨ ××ª ×”×˜×§×¡×˜×™× ×”××§×•×¨×™×™×.
        """

        cleaned = [(t or "").strip() for t in texts]
        if not any(cleaned):
            return cleaned

        if InferenceClient is None or not self.hf_token:
            return cleaned

        try:
            client = InferenceClient(provider="hf-inference", api_key=self.hf_token)
        except Exception as e:
            print(f"âš ï¸ HF translation client init failed: {e}")
            return cleaned

        results: list[str] = []
        for idx, text in enumerate(cleaned):
            if not text:
                results.append("")
                continue
            try:
                prompt = f"translate English to Hebrew: {text}"
                out = client.translation(prompt, model=self.hf_translation_model)
                if isinstance(out, str):
                    translated = out
                elif isinstance(out, dict) and "translation_text" in out:
                    translated = out["translation_text"]
                elif isinstance(out, list) and out and isinstance(out[0], dict):
                    translated = out[0].get("translation_text") or out[0].get(
                        "generated_text", ""
                    )
                else:
                    translated = text

                results.append(self._clean_translation_output(translated))
            except Exception as e:
                print(f"âš ï¸ HF single translation failed (idx={idx}): {e}")
                results.append(text)

        return results