import os
import requests

from langchain_community.llms import Ollama

try:
    from huggingface_hub import InferenceClient
except ImportError:  # huggingface-hub ××•×¤×¦×™×•× ×œ×™
    InferenceClient = None

class AIService:
    def __init__(self):
        self.llm = None
        self.is_active = False
        self.hf_client = None
        self.hf_active = False
        # ×˜×•×§×Ÿ Hugging Face â€“ ×ª×•××š ×’× HF_TOKEN ×•×’× HUGGINGFACEHUB_API_TOKEN
        self.hf_token = os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACEHUB_API_TOKEN")
        # ××•×“×œ ×ª×¨×’×•× ×‘×¨×™×¨×ª ××—×“×œ â€“ ×™×¦×™×‘ ×•× ×ª××š ×‘-HF Inference API
        self.hf_translation_model = os.getenv(
            "HF_TRANSLATION_MODEL", "Helsinki-NLP/opus-mt-en-he"
        )
        
        # × ×™×¡×™×•×Ÿ ×˜×¢×™× ×” "×‘×˜×•×—" - ×œ× ×™×¤×™×œ ××ª ×”×©×¨×ª ×× ×”×¡×¤×¨×™×™×” ×œ× ××•×ª×§× ×ª
        try:
            # ×× ×¡×™× ×œ×™×™×‘× ×¨×§ ×× ×§×™×™×
            from langchain_ollama import OllamaLLM
            self.llm = OllamaLLM(model="llama3.2:1b")
            self.is_active = True
            print("âœ… AI Service initialized (Ready to connect to Docker/Local Ollama)")
            
        except ImportError:
            print("âš ï¸ langchain-ollama not installed. AI Service running in MOCK mode.")
        except Exception as e:
            print(f"âš ï¸ AI Service warning: {e}. Running in MOCK mode.")

        # --- Hugging Face Inference API ×œ×“×™×¨×•×’ ×—×“×©×•×ª ---
        try:
            if InferenceClient is not None:
                # ××—×–×™×§×™× client (×’× ×œ×“×¨×•×’, ×’× ×œ×ª×¨×’×•× ×× × ×¨×¦×”), ××‘×œ ×œ×“×™×¨×•×’ × ×©×ª××© ×‘-HTTP ×™×©×™×¨
                self.hf_client = InferenceClient(
                    provider="hf-inference",
                    api_key=self.hf_token,
                ) if self.hf_token else None
                self.hf_active = bool(self.hf_token)
                if self.hf_active:
                    print("âœ… HF Client initialized (token detected) for news AI features")
                else:
                    print("âš ï¸ HF token not set â€“ news AI in MOCK mode")
            else:
                print("âš ï¸ huggingface-hub not installed. News ranking in MOCK mode.")
        except Exception as e:
            print(f"âš ï¸ HF Client init failed: {e}. News ranking in MOCK mode.")

    def analyze_stock(self, symbol: str, price: float) -> str:
        # ×× ××™×Ÿ ×œ× ×• ××•×“×œ ×××™×ª×™, × ×—×–×™×¨ ×ª×©×•×‘×ª ×“××”
        if not self.is_active or not self.llm:
            return (f"[MOCK Analysis] The AI service is currently optimized via Docker. "
                    f"Mock insight: {symbol} at ${price} shows stable trends. "
                    f"Consider market volatility before investing.")

        # × ×™×¡×™×•×Ÿ ×¤× ×™×™×” ×œ××•×“×œ ×”×××™×ª×™ (×× ×”×•× ××•×ª×§×Ÿ)
        try:
            prompt = f"Analyze the stock {symbol} at {price} USD. Is it risky? Answer in 2 short sentences."
            return self.llm.invoke(prompt)
        except Exception as e:
            print(f"âŒ Connection to Ollama failed: {e}")
            return "AI Service is offline. Please check your Docker container."
    
    def generate_investment_plan(self, prompt: str) -> str:
        """
        ×™×¦×™×¨×ª ×ª×›× ×™×ª ×”×©×§×¢×” ××•×ª×××ª ××™×©×™×ª ×‘×××¦×¢×•×ª AI
        """
        # ×× ××™×Ÿ ××•×“×œ ×××™×ª×™, × ×—×–×™×¨ ×ª×•×›× ×™×ª ×“××”
        if not self.is_active or not self.llm:
            return """
ğŸ“Š INVESTMENT PLAN - Based on Your Profile

ğŸ¯ Top 5 Stock Recommendations:
1. NVDA (40%) - Nvidia - Strong tech growth, high dividend potential
2. AAPL (25%) - Apple - Stable blue-chip, defensive position
3. JNJ (20%) - Johnson & Johnson - Dividend stock, low risk
4. MSFT (10%) - Microsoft - Cloud growth exposure
5. Cash Reserve (5%) - Liquidity buffer for short-term opportunities

ğŸ“ˆ Expected Performance:
- Annual Return Projection: 6-8% (Conservative approach)
- Risk Level: Low-Medium
- Volatility Index: Moderate

â° Implementation Timeline:
Week 1: Allocate 50% of capital
Week 2-3: Allocate remaining 50%
Monitor quarterly, rebalance annually

âœ… Diversification Score: 85% - Well-balanced across sectors
ğŸ›¡ï¸ Risk Assessment: CONSERVATIVE - Suitable for your profile
            """
        
        # × ×™×¡×™×•×Ÿ ×¤× ×™×™×” ×œ××•×“×œ ×”×××™×ª×™ (×× Ollama ×¨×¥)
        try:
            print(f"ğŸ¤– AI Service: Generating investment plan with Llama3...")
            response = self.llm.invoke(prompt)
            return str(response).strip()
        except Exception as e:
            print(f"âŒ AI Error: {e}")
            print(f"âš ï¸ Falling back to mock mode...")
            return """
ğŸ“Š INVESTMENT PLAN - MOCK MODE (AI Unavailable)

ğŸ¯ Top 5 Stock Recommendations:
1. NVDA (40%) - Nvidia - Strong tech growth
2. AAPL (25%) - Apple - Stable blue-chip
3. JNJ (20%) - Johnson & Johnson - Dividend stock
4. MSFT (10%) - Microsoft - Cloud growth
5. Cash Reserve (5%) - Liquidity buffer

ğŸ“ˆ Expected Performance:
- Annual Return: 6-8%
- Risk Level: Low-Medium
- Diversification: 85%

âš ï¸ Note: This is a mock analysis. Ensure Ollama is running for real AI recommendations.
            """

    def rank_news_for_stock(self, symbol: str, news_items: list[dict]) -> list[dict]:
        """×“×™×¨×•×’ ×—×“×©×•×ª ×œ×¤×™ ×—×©×™×‘×•×ª ×¢×‘×•×¨ ×× ×™×” ××¡×•×™××ª.

        news_items: ×¨×©×™××” ×©×œ ××•×‘×™×™×§×˜×™× ×¢× ×”××¤×ª×—×•×ª: title, summary, url, published_at
        ××—×–×™×¨: ××•×ª×” ×¨×©×™××”, ×¢× ×©×“×” importance_score ×•×××•×™×™× ×ª ××”×›×™ ×—×©×•×‘ ×œ×¤×—×•×ª ×—×©×•×‘.
        """

        # ××¦×‘ MOCK â€“ ×× HF ×œ× ×–××™×Ÿ/××™×Ÿ ×˜×•×§×Ÿ, × ×‘×¦×¢ ×“×™×¨×•×’ ×¤×©×•×˜ ×œ×¤×™ ××•×¨×š ×”×˜×§×¡×˜
        if not self.hf_active or not getattr(self, "self.hf_token", None):
            ranked = []
            for item in news_items:
                text = f"{item.get('title', '')}. {item.get('summary', '')}"
                score = min(len(text) / 400.0, 1.0)  # ×”×¢×¨×›×” ×’×¡×” ×‘×™×Ÿ 0 ×œ-1
                enriched = dict(item)
                enriched["importance_score"] = round(float(score), 3)
                ranked.append(enriched)

            ranked.sort(key=lambda x: x["importance_score"], reverse=True)
            return ranked

        ranked_items: list[dict] = []

        api_url = "https://api-inference.huggingface.co/models/facebook/bart-large-mnli"
        headers = {"Authorization": f"Bearer {self.hf_token}"}
        labels = ["very important", "somewhat important", "not important"]

        for item in news_items:
            text = f"[Ticker: {symbol.upper()}] {item.get('title', '')}. {item.get('summary', '')}".strip()
            if not text:
                continue

            try:
                resp = requests.post(
                    api_url,
                    headers=headers,
                    json={
                        "inputs": text,
                        "parameters": {
                            "candidate_labels": labels,
                            "multi_label": False,
                        },
                    },
                    timeout=25,
                )
                if resp.status_code != 200:
                    raise ValueError(f"status={resp.status_code}, body={resp.text[:100]}")

                data = resp.json()
                label_scores: dict[str, float] = {}
                if isinstance(data, dict) and "labels" in data and "scores" in data:
                    for lbl, sc in zip(data["labels"], data["scores"]):
                        label_scores[lbl] = float(sc)
                else:
                    raise ValueError("Unexpected HF zero-shot result format")

                score = (
                    1.0 * label_scores.get("very important", 0.0)
                    + 0.6 * label_scores.get("somewhat important", 0.0)
                    + 0.1 * label_scores.get("not important", 0.0)
                )

            except Exception as e:
                print(f"âš ï¸ HF ranking failed for news item: {e}")
                # fallback ×‘××§×¨×” ×©×œ ×©×’×™××”: ×“×™×¨×•×’ ×¤×©×•×˜ ×œ×¤×™ ××•×¨×š ×”×˜×§×¡×˜
                score = min(len(text) / 400.0, 1.0)

            enriched = dict(item)
            enriched["importance_score"] = round(float(score), 3)
            ranked_items.append(enriched)

        ranked_items.sort(key=lambda x: x["importance_score"], reverse=True)
        return ranked_items